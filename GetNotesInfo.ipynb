{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d1a3ab-f45f-481f-ba66-940bfd998f11",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "26dba06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a2f5ccca-2936-4389-9eae-c21016560e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "vault_path = r\"C:\\Users\\nonak\\Documents\\Thougts\"\n",
    "second_path = r\"C:\\Users\\nonak\\Documents\\MyObsidianSetup\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c51556-af50-49ec-b5c6-7d3c942defac",
   "metadata": {},
   "source": [
    "# Property Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41fc35-6c79-4346-9c2a-456d8232738e",
   "metadata": {},
   "source": [
    "## Get Meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84a97817-d014-40d3-8d09-756c13b8e6ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_file_metadata(file_path):\n",
    "    \"\"\"Get enhanced file metadata with relative path\"\"\"\n",
    "    stat = os.stat(file_path)\n",
    "    return {\n",
    "        'created': datetime.fromtimestamp(stat.st_ctime),\n",
    "        'modified': datetime.fromtimestamp(stat.st_mtime),\n",
    "        'size_kb': stat.st_size / 1024,\n",
    "        'rel_path': os.path.relpath(file_path, start=directory_to_scan)\n",
    "    }\n",
    "\n",
    "def analyze_frontmatter(directory):\n",
    "    \"\"\"Extract and analyze all frontmatter data with statistical insights\"\"\"\n",
    "    property_stats = defaultdict(Counter)\n",
    "    file_count = 0\n",
    "    dates = []\n",
    "    sizes = []\n",
    "    property_presence = Counter()\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_count += 1\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                if content.startswith(\"---\"):\n",
    "                    end_of_header = content.find(\"---\", 3)\n",
    "                    if end_of_header != -1:\n",
    "                        header = content[3:end_of_header].strip()\n",
    "                        try:\n",
    "                            metadata = get_file_metadata(file_path)\n",
    "                            dates.append(metadata['created'])\n",
    "                            sizes.append(metadata['size_kb'])\n",
    "                            \n",
    "                            data = yaml.safe_load(header) or {}\n",
    "                            for prop, value in data.items():\n",
    "                                property_presence[prop] += 1\n",
    "                                if isinstance(value, list):\n",
    "                                    for item in value:\n",
    "                                        property_stats[prop][str(item)] += 1\n",
    "                                else:\n",
    "                                    property_stats[prop][str(value)] += 1\n",
    "                        except yaml.YAMLError:\n",
    "                            pass\n",
    "    \n",
    "    # Calculate date statistics\n",
    "    date_stats = {}\n",
    "    if dates:\n",
    "        sorted_dates = sorted(dates)\n",
    "        date_stats = {\n",
    "            'oldest': min(dates),\n",
    "            'newest': max(dates),\n",
    "            'timespan': max(dates) - min(dates),\n",
    "            'median': sorted_dates[len(sorted_dates)//2],\n",
    "            'total_files': file_count,\n",
    "            'files_per_day': file_count / (max(dates) - min(dates)).days if len(dates) > 1 else 0\n",
    "        }\n",
    "    \n",
    "    # Calculate size statistics\n",
    "    size_stats = {}\n",
    "    if sizes:\n",
    "        size_stats = {\n",
    "            'total_mb': sum(sizes) / 1024,\n",
    "            'avg_kb': sum(sizes) / len(sizes),\n",
    "            'largest_kb': max(sizes),\n",
    "            'smallest_kb': min(sizes)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'property_stats': dict(property_stats),\n",
    "        'property_presence': property_presence,\n",
    "        'date_stats': date_stats,\n",
    "        'size_stats': size_stats,\n",
    "        'total_files': file_count\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68836be9-0a6b-44d8-8bf7-bba9ce886b09",
   "metadata": {},
   "source": [
    "## generate report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "13c05d21-de4f-4c64-8c84-5d3fdd2c43c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical report generated: C:\\Users\\nonak\\Documents\\Thoughts\\PropertyAnalysis.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_statistical_report(data, output_file=\"markdown_stats_report.md\"):\n",
    "    \"\"\"Generate a statistics-focused markdown report\"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as md:\n",
    "        # Report Header\n",
    "        md.write(\"# Markdown Statistics Report\\n\\n\")\n",
    "        \n",
    "        md.write(f\"> **Analysis performed:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        md.write(f\"> **Total files analyzed:** {data['total_files']:,}\\n\\n\")\n",
    "        \n",
    "        if data['date_stats']:\n",
    "            stats = data['date_stats']\n",
    "            md.write(f\"> **Date range:** {stats['oldest'].strftime('%Y-%m-%d')} to {stats['newest'].strftime('%Y-%m-%d')}\\n\")\n",
    "            md.write(f\"> **Timespan:** {stats['timespan'].days} days\\n\")\n",
    "            md.write(f\"> **Average files per day:** {stats['files_per_day']:.2f}\\n\\n\")\n",
    "            md.write(f\"[[_index_notas|Acessar Index todas as notas]]\")\n",
    "        else:\n",
    "            md.write(\"*No date information available*\\n\")\n",
    "        \n",
    "        # # Size Analysis\n",
    "        # md.write(\"\\n## 📦 Size Analysis\\n\")\n",
    "        # if data['size_stats']:\n",
    "        #     stats = data['size_stats']\n",
    "        #     md.write(f\"- **Total content size:** {stats['total_mb']:.2f} MB\\n\")\n",
    "        #     md.write(f\"- **Average file size:** {stats['avg_kb']:.1f} KB\\n\")\n",
    "        #     md.write(f\"- **Largest file:** {stats['largest_kb']:.1f} KB\\n\")\n",
    "        #     md.write(f\"- **Smallest file:** {stats['smallest_kb']:.1f} KB\\n\")\n",
    "        #     md.write(f\"- **Size range:** {stats['largest_kb']/stats['smallest_kb']:.1f}x variation\\n\")\n",
    "        # else:\n",
    "        #     md.write(\"*No size information available*\\n\")\n",
    "        \n",
    "        # Property Prevalence\n",
    "        md.write(\"\\n\\n\\n## Property Counts\\n\")\n",
    "        md.write(\"| Property | Files | Coverage |\\n\")\n",
    "        md.write(\"|----------|-------|----------|\\n\")\n",
    "        for prop, count in data['property_presence'].most_common(15):\n",
    "            coverage = (count / data['total_files']) * 100\n",
    "            md.write(f\"| `{prop}` | {count} | {coverage:.1f}% |\\n\")\n",
    "        \n",
    "        # Property Value Analysis\n",
    "        md.write(\"\\n## By Property\\n\")\n",
    "        for prop, counter in data['property_stats'].items():\n",
    "            total = sum(counter.values())\n",
    "            unique = len(counter)\n",
    "            value_counts = f\"\\n> **Present in:** {data['property_presence'][prop]} files ({data['property_presence'][prop]/data['total_files']:.1%})\\n\"\n",
    "            md.write(f\"\\n##### `{prop}` {value_counts}\")\n",
    "                \n",
    "            # Show value distribution if not too large\n",
    "            if unique <= 15:\n",
    "                md.write(\"\\n**Value Distribution:**\\n\")\n",
    "                for value, count in counter.most_common():\n",
    "                    md.write(f\"- `{value}`: {count} ({count/total:.1%})\\n\")\n",
    "            else:\n",
    "                top_values = counter.most_common(5)\n",
    "                others = total - sum(count for _, count in top_values)\n",
    "                md.write(\"\\n> **Top Values:**\\n\")\n",
    "                for value, count in top_values:\n",
    "                    md.write(f\"- `{value}`: {count} ({count/total:.1%})\\n\")\n",
    "                md.write(f\"\\n> *Others ({unique-5} values)*: {others} ({others/total:.1%})\\n\")\n",
    "            \n",
    "            # Calculate Gini coefficient for inequality\n",
    "            # if unique > 1:\n",
    "            #     sorted_counts = sorted(counter.values())\n",
    "            #     n = len(sorted_counts)\n",
    "            #     gini = sum(abs(x-y) for x in sorted_counts for y in sorted_counts) / (2*n*sum(sorted_counts))\n",
    "            #     md.write(f\"- **Value inequality (Gini):** {gini:.3f} (0=equal, 1=unequal)\\n\")\n",
    "        \n",
    "        # Recommendations Section\n",
    "        # md.write(\"\\n## 🚀 Recommendations\\n\")\n",
    "        # md.write(\"### Based on your metadata patterns:\\n\")\n",
    "        \n",
    "        # # Property standardization opportunities\n",
    "        # common_props = [p for p, c in data['property_presence'].most_common(5) if c/data['total_files'] > 0.7]\n",
    "        # if common_props:\n",
    "        #     md.write(\"- These properties are nearly universal and could be enforced:\\n\")\n",
    "        #     for prop in common_props:\n",
    "        #         md.write(f\"  - `{prop}` (in {data['property_presence'][prop]/data['total_files']:.0%} of files)\\n\")\n",
    "        \n",
    "        # # Underused properties\n",
    "        # rare_props = [p for p, c in data['property_presence'].items() if 0 < c/data['total_files'] < 0.1]\n",
    "        # if rare_props:\n",
    "        #     md.write(\"\\n- These properties are rarely used and might need review:\\n\")\n",
    "        #     for prop in rare_props[:5]:\n",
    "        #         md.write(f\"  - `{prop}` (only {data['property_presence'][prop]} files)\\n\")\n",
    "        \n",
    "        # # High-value-diversity properties\n",
    "        # diverse_props = []\n",
    "        # for prop, counter in data['property_stats'].items():\n",
    "        #     unique = len(counter)\n",
    "        #     total = sum(counter.values())\n",
    "        #     if unique > 10 and total/unique < 3:\n",
    "        #         diverse_props.append((prop, unique))\n",
    "        \n",
    "        # if diverse_props:\n",
    "        #     md.write(\"\\n- These properties have many unique values with low repetition:\\n\")\n",
    "        #     for prop, unique in sorted(diverse_props, key=lambda x: x[1], reverse=True)[:3]:\n",
    "        #         md.write(f\"  - `{prop}` ({unique} unique values)\\n\")\n",
    "        #         values_sample = \", \".join(f\"`{v}`\" for v, _ in data['property_stats'][prop].most_common(3))\n",
    "        #         md.write(f\"    *Sample values:* {values_sample}\\n\")\n",
    "\n",
    "# Configuration\n",
    "directory_to_scan = r\"C:\\Users\\nonak\\Documents\\Thoughts\"\n",
    "output_file = r\"C:\\Users\\nonak\\Documents\\Thoughts\\PropertyAnalysis.md\"\n",
    "\n",
    "# Run analysis and generate report\n",
    "data = analyze_frontmatter(directory_to_scan)\n",
    "generate_statistical_report(data, output_file)\n",
    "print(f\"Statistical report generated: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1a848-07d8-4256-8bd5-1775b28739fa",
   "metadata": {},
   "source": [
    "# create a index note with all notes by folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ba18bb05-23cb-4c71-880e-9052a4a4df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo em: C:\\Users\\nonak\\Documents\\Thoughts\\_index_notas.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def contar_palavras(texto):\n",
    "    return len(texto.split())\n",
    "\n",
    "def listar_notas_markdown_organizadas(pasta_raiz):\n",
    "    notas_por_pasta = defaultdict(list)\n",
    "    contagem_palavras_por_pasta = defaultdict(int)\n",
    "    total_palavras_geral = 0\n",
    "\n",
    "    for raiz, _, arquivos in os.walk(pasta_raiz):\n",
    "        caminho_relativo = os.path.relpath(raiz, pasta_raiz)\n",
    "        for arquivo in arquivos:\n",
    "            if arquivo.endswith(\".md\"):\n",
    "                caminho_completo = os.path.join(raiz, arquivo)\n",
    "                nome_nota = os.path.splitext(arquivo)[0]\n",
    "                \n",
    "                with open(caminho_completo, 'r', encoding='utf-8') as f:\n",
    "                    conteudo = f.read()\n",
    "                    palavras = contar_palavras(conteudo)\n",
    "                    total_palavras_geral += palavras\n",
    "                    contagem_palavras_por_pasta[caminho_relativo] += palavras\n",
    "                    notas_por_pasta[caminho_relativo].append((nome_nota, palavras))\n",
    "\n",
    "    return notas_por_pasta, contagem_palavras_por_pasta, total_palavras_geral\n",
    "\n",
    "def salvar_em_markdown(notas_organizadas, contagem_por_pasta, total_geral, caminho_saida):\n",
    "    with open(caminho_saida, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# Índice de Notas Markdown\\n\\n\")\n",
    "        for pasta, notas in sorted(notas_organizadas.items()):\n",
    "            titulo_pasta = pasta if pasta != '.' else '[raiz]'\n",
    "            f.write(f\"### {titulo_pasta} — {len(notas)} notas\\n\")\n",
    "            for nome_nota, palavras in sorted(notas):\n",
    "                f.write(f\"- [{nome_nota}] — {palavras} palavras\\n\")\n",
    "            f.write(f\"\\n**Total nesta pasta**: {contagem_por_pasta[pasta]} palavras\\n\\n\")\n",
    "\n",
    "        f.write(\"---\\n\")\n",
    "        f.write(f\"## 📊 Total geral: {total_geral} palavras\\n\")\n",
    "\n",
    "# Caminhos\n",
    "caminho_da_pasta = r\"C:\\Users\\nonak\\Documents\\Thoughts\"\n",
    "caminho_arquivo_saida = os.path.join(caminho_da_pasta, \"_index_notas.md\")\n",
    "\n",
    "# Execução\n",
    "notas_organizadas, contagem_por_pasta, total_palavras = listar_notas_markdown_organizadas(caminho_da_pasta)\n",
    "salvar_em_markdown(notas_organizadas, contagem_por_pasta, total_palavras, caminho_arquivo_saida)\n",
    "\n",
    "print(f\"Arquivo salvo em: {caminho_arquivo_saida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b0c86-e87b-464e-9264-4aaa6377809b",
   "metadata": {},
   "source": [
    "# daily Quote extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab3464e-8b13-432e-addc-f08b5d466f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo gerado com sucesso em: C:\\Users\\nonak\\Documents\\Thoughts\\quotes.md\n"
     ]
    }
   ],
   "source": [
    "def extract_quotes_to_markdown(folder_path, output_path):\n",
    "    quote_pattern = re.compile(r\"> \\[!quote\\] (.+?)\\n> — (.+)\")\n",
    "    all_entries = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".md\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                matches = quote_pattern.findall(content)\n",
    "                for quote, author in matches:\n",
    "                    entry = f\"> [!quote] {quote.strip()}\\n> — {author.strip()}\\n*Origem: {filename}*\\n\"\n",
    "                    all_entries.append(entry)\n",
    "\n",
    "    # Write to a markdown file\n",
    "    with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(\"# Coletânea de Citações\\n\\n\")\n",
    "        for entry in all_entries:\n",
    "            out_file.write(entry + \"\\n\")\n",
    "\n",
    "# Uso\n",
    "folder = r\"C:\\Users\\nonak\\Documents\\Thoughts\\Calendar\\DAILY\"\n",
    "output_md = r\"C:\\Users\\nonak\\Documents\\Thoughts\\quotes.md\"\n",
    "extract_quotes_to_markdown(folder, output_md)\n",
    "print(f\"Arquivo gerado com sucesso em: {output_md}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05ba42-21d2-408b-a6df-1f4d2cd52665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digite o caminho do projeto:  C:\\Users\\nonak\\AppData\\Local\n"
     ]
    }
   ],
   "source": [
    "def buscar_arquivos_por_palavra(caminho_raiz, palavra_chave):\n",
    "    caminhos_encontrados = []\n",
    "\n",
    "    for raiz, _, arquivos in os.walk(caminho_raiz):\n",
    "        for nome_arquivo in arquivos:\n",
    "            caminho_completo = os.path.join(raiz, nome_arquivo)\n",
    "            try:\n",
    "                with open(caminho_completo, 'r', encoding='utf-8') as arquivo:\n",
    "                    conteudo = arquivo.read()\n",
    "                    if palavra_chave in conteudo:\n",
    "                        caminhos_encontrados.append(caminho_completo)\n",
    "            except (UnicodeDecodeError, PermissionError, FileNotFoundError):\n",
    "                # Ignora arquivos que não podem ser lidos como texto\n",
    "                continue\n",
    "\n",
    "    return caminhos_encontrados\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    caminho = input(\"Digite o caminho do projeto: \").strip()\n",
    "    palavra = input(\"Digite a palavra a buscar: \").strip()\n",
    "    encontrados = buscar_arquivos_por_palavra(caminho, palavra)\n",
    "\n",
    "    print(\"\\nArquivos contendo a palavra:\")\n",
    "    for arquivo in encontrados:\n",
    "        print(arquivo)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae6afe-4b74-4bfa-9997-2cd586137297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
